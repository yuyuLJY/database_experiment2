package cluster;

import java.io.IOException;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.Map;

import org.apache.hadoop.mapred.JobConf;



public class Main {
	public static final String HDFS = "hdfs://192.168.126.132:9000";
	public static void main(String[] args) throws ClassNotFoundException, IOException, InterruptedException {
		//如果还变化就不停下来
		Map<String,String> path = new HashMap<String,String>();
		path.put("Step1Input", HDFS+"/BigDataAnaly/experiment2/Data/small_kmean.txt");
		//path.put("Step1Input", HDFS+"/BigDataAnaly/experiment2/kmeanData/large_kmean_data.txt");
		path.put("Step1Output", HDFS+"/BigDataAnaly/experiment2/kmeanResult/1");
		ArrayList<String> clusterList = new ArrayList<String>();
		info INFO = new info(clusterList);
		String input="",output="";
		
		Step1.run(path);
		//Step1完成后簇点集的状态:
		int count=1;
		/*
		info.clusterList.add("#0,4.632092238470191,3.957052868391451,1.195748031496063,0.1755511811023622,3.455877390326209,1.4732845894263218,1.8108155230596175,1.8931327334083239,1.919476940382452,1.538880764904387,0.006299212598425197,1.5825309336332958,0.7849268841394825,1.9944094488188977,1.7952643419572554,5.507047244094488,1.0769741282339707,0.0591169853768279,0.004431946006749157,0.18940944881889762,0.13315523059617548,0.04705286839145107,0.05262654668166479,0.03282339707536558,4.799572553430822,0.004983127109111361,1.2473228346456693,0.6843194600674916,1.184493813273341,0.006704161979752531,1.2497919010123735,3.862609673790776,1.5644994375703036,1.936962879640045,2.3925365579302587,0.0011417322834645668,1.9153599550056244,3.3390776152980877,1.7987739032620922,1.2572665916760406,1.6861586051743531,1.8104443194600675,2.08888638920135,0.25471878515185603,0.0,0.6308605174353206,2.8561867266591676,0.0,2.207564679415073,50.76228346456693,0.006709786276715411,2.5940326209223845,0.2809392575928009,1.176917885264342,0.011951631046119236,0.5170866141732283,0.0409673790776153,0.03324521934758155,1.009206974128234,2.101974128233971,0.013554555680539933,1.0041394825646794,1.3249100112485939,1.3235095613048369,0.014038245219347581,9.231456692913385,2.486962879640045,0.0609111361079865");
		info.clusterList.add("#1,1.2061149962539026,3.63058603266337,1.7236924007040388,0.0337051260590242,0.13805303693928397,0.0753705061050825,0.13530156490987424,0.19884220397297608,0.20032601611878298,0.14464974394895555,0.0,0.07553140139800132,0.1613893552324287,0.07137412706177566,0.0376446229280668,0.11572597093808457,0.048588753256504476,0.001048257211440793,1.6252049789779735E-4,0.014212417541162379,0.004111768596814273,0.002523943332352793,5.428184629786432E-4,0.009354679858997217,0.37582215056874047,0.0,1.3270773776342542,0.11649144248318319,4.0,0.0,0.07339750726060325,0.4020887134389825,1.044849156599876,0.3035297826938423,0.18220823101313655,0.0,0.30089695062789795,0.145060920808637,1.8256610114950749,1.0951346238544337,1.9425555048130445,0.09847767049619133,2.2663174642901835,0.033456469697240566,141.13702753259756,0.03383514245734243,0.42318549927109556,0.9465632602912042,0.13670086639677428,13.5192578663984,1.0,0.9143240691232182,1.0401311865459031E-4,1.6077324002489815,1.0401311865459031E-4,0.48725432995236523,0.16540686194046225,0.05842936940421611,0.18281768288025327,0.07513647658810968,0.0,0.054992060873677695,0.15173401245232054,0.3956675285670405,0.0,3.395264477732254,0.38887092134495466,1.0401311865459031E-4");
		info.clusterList.add("#2,4.7458704114515085,3.1062198755928794,1.559447098148066,0.14706115502366715,0.00529853264936241,1.639655940686221,1.8154227355874266,1.8385469901716214,1.8835812147410067,0.09354255220763186,0.02689682424341422,1.547403340663881,0.06280349608270107,2.09311797297346,1.8546647865873798,0.03412668195231981,1.1911014918525227,0.06642533110574365,0.015447597794349913,0.2667180325466707,0.1903165907788837,0.04295696916485805,0.0920952594857727,0.05435575055639697,5.137346277695237,0.02973856248401072,1.925974280227099,0.673859491297627,1.1974791884110887,0.014711940705438097,1.1052692264730857,3.5886800089840247,1.4294555897327492,1.9262739478902555,2.292046255715605,0.0021066816880837965,1.926018119384234,0.00529853264936241,1.8328352884796701,1.1164343992053702,1.7543439800429743,1.966593361190215,1.5167033194048924,0.0733339018411245,0.0,0.6337610753925406,2.806972587915526,0.0,2.480234545496037,14.556417992550946,0.007647831042687328,2.5556919440444203,1.0872873650744845,1.129595154391903,0.02232614170977517,0.5254303143567834,0.024350549911180668,0.015182160705942548,0.9967288782340387,1.937933362079009,0.04878097116344319,1.0992896855471308,1.2798133292657001,1.3290831370580203,0.05525715569146362,10.229257172506482,2.278479537923273,0.1989450977613204");
		int count = 7;
		*/
		int round=0;//debug阶段的轮次
		while(round<100000) {
			System.out.println("=====================新的一轮"+String.valueOf(count)+"=====================");
			round++;
			info.isStop = true;
			//否则，继续进行寻找
			input = HDFS+"/BigDataAnaly/experiment2/kmeanResult/"+String.valueOf(count);
			count++;//初始化新一轮的输出
			output = HDFS+"/BigDataAnaly/experiment2/kmeanResult/"+String.valueOf(count);
			info.setChangeCount(0);//重新开始计算
			Step2.run(input,output);
			System.out.println("改变的数量:"+info.changeNum);
			//如果step2中没有一个点反对停止，则停止
			if(info.isStop==true) {//已经可以停止了
				break;
			}
		}
		
		//Step3:对分好类的内容，进行排序，得到按照顺序的数字0,1,2
		//mapreduce好像有这个机制，把需要当成key
		//output = HDFS+"/BigDataAnaly/experiment2/kmeanResult/7";
		Step3.run(output,HDFS+"/BigDataAnaly/experiment2/kmeanResult/Last");
		System.out.println("end");
		
	}
	
    public static JobConf config() {
        JobConf conf = new JobConf(Main.class);
        conf.setJobName("Recommend");
        conf.addResource("classpath:/hadoop/core-site.xml");
        conf.addResource("classpath:/hadoop/hdfs-site.xml");
        conf.addResource("classpath:/hadoop/mapred-site.xml");
        return conf;
    }
}
